Introduction:
"Live Speech Portraits" aims to revolutionize digital communication by generating real-time photorealistic talking-head animations from audio inputs. This project leverages advanced machine learning techniques to enhance the visual realism of virtual interactions, making them as natural as face-to-face conversations.
Implementation steps on Local:
Requirements and Setup:
Environment Setup:
conda create -n LSP python=3.6
conda activate LSP
Installation
Clone the repository and install dependencies:
git clone https://github.com/YuanxunLu/LiveSpeechPortraits.git
cd LiveSpeechPortraits
sudo apt-get install ffmpeg  # For Linux users
pip install -r requirements.txt
Demo
Download pre-trained models and data into the data folder from the provided Google Drive links.
Run the Demo:
python demo.py
Results will be saved in the results folder.

